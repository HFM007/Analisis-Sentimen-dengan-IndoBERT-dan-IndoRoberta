{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411169ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, IntervalStrategy, EarlyStoppingCallback\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Langkah 2 Memasukan dataset dan tahap pengolahan data\n",
    "df = pd.read_csv(\"label_manual - Sheet1.csv\")\n",
    "\n",
    "# Perbaiki baris rename: ganti 'komentar' ke 'text'\n",
    "# dan pastikan kolom label Anda akan diolah menjadi 'labels'\n",
    "df.rename(columns={'komentar': 'text'}, inplace=True) # Hanya ganti 'komentar' ke 'text'\n",
    "\n",
    "print(f\"Nilai kosong pada kolom text: {df['text'].isna().sum()}\")\n",
    "print(f\"Nilai kosong pada kolom label: {df['label'].isna().sum()}\") # Kolom 'label' masih ada di sini\n",
    "\n",
    "df = df.dropna(subset=['text'])\n",
    "df = df.dropna(subset=['label']) # Ini akan menghapus baris dengan NaN di kolom 'label' asli\n",
    "\n",
    "print(f\"DataFrame shape after dropping initial NA: {df.shape}\")\n",
    "\n",
    "label_map = {'Netral': 0, 'Positif': 1, 'Negatif': 2} # Perhatikan perubahan kapitalisasi sesuai data Anda\n",
    "df['label'] = df['label'].map(label_map)\n",
    "\n",
    "# *** PERBAIKAN UTAMA: Ganti nama kolom 'label' menjadi 'labels' dan pastikan tipe datanya int\n",
    "df.rename(columns={'label': 'labels'}, inplace=True)\n",
    "df['labels'] = df['labels'].astype(int) # Pastikan label bertipe integer\n",
    "# *** AKHIR PERBAIKAN UTAMA\n",
    "\n",
    "print(f\"DataFrame shape after label mapping and renaming to 'labels': {df.shape}\")\n",
    "\n",
    "# Langkah 3 Inisialisasi model (tokenizer tetap sama untuk semua fold)\n",
    "model_name = \"cahya/roberta-base-indonesian-522M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Langkah 4 Tokenisasi Data (Tokenisasi dilakukan sekali pada seluruh dataset awal)\n",
    "def tokenize_function(examples):\n",
    "  texts = [str(text) if text is not None else \"\" for text in examples['text']]\n",
    "  return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Buat dataset Hugging Face dari DataFrame\n",
    "full_dataset = Dataset.from_pandas(df)\n",
    "full_dataset = full_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Optional: Tampilan contoh tokenisasi (tetap bisa di sini)\n",
    "sample_idx = 0\n",
    "sample_text = full_dataset[sample_idx]['text']\n",
    "sample_tokens = full_dataset[sample_idx]\n",
    "\n",
    "token_ids = sample_tokens['input_ids']\n",
    "token_words = tokenizer.convert_ids_to_tokens(token_ids[:30])\n",
    "\n",
    "token_df = pd.DataFrame({\n",
    "  'Position': range(len(token_words)),\n",
    "  'Token': token_words,\n",
    "  'ID': token_ids[:len(token_words)],\n",
    "  'Attention Mask': sample_tokens['attention_mask'][:len(token_words)]\n",
    "})\n",
    "\n",
    "token_df['Token'] = token_df['Token'].str.replace('Ä ', '', regex=False)\n",
    "\n",
    "print(f\"\\nText asli sebelum tokenisasi: {sample_text}\\\\n\")\n",
    "print(\"Hasil tokenisasi:\")\n",
    "display(token_df)\n",
    "\n",
    "# ==============================================================================\n",
    "# PENAMBAHAN CROSS-VALIDATION\n",
    "# ==============================================================================\n",
    "\n",
    "num_folds = 5 # Anda bisa mengubah jumlah fold sesuai kebutuhan\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42) # random_state untuk reproduktifitas\n",
    "\n",
    "all_fold_metrics = []\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "all_texts = [] # *** PERBAIKAN: List baru untuk mengumpulkan semua teks dari set pengujian\n",
    "\n",
    "# Iterasi untuk setiap fold\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(full_dataset)):\n",
    "    print(f\"\\n===== Training Fold {fold + 1}/{num_folds} =====\")\n",
    "\n",
    "    # Split dataset untuk fold saat ini\n",
    "    train_dataset = full_dataset.select(train_index)\n",
    "    test_dataset = full_dataset.select(test_index)\n",
    "\n",
    "    # Langkah 6 Load Model (model perlu diinisialisasi ulang untuk setiap fold agar tidak ada informasi bocor)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_map))\n",
    "\n",
    "    # Langkah 7 Set training config & trainer\n",
    "    # Pastikan output_dir unik untuk setiap fold jika Anda ingin menyimpan model per fold\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_fold_{fold + 1}\", # Direktori output unik per fold\n",
    "        eval_strategy=IntervalStrategy.EPOCH,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=1, # Mengurangi epoch untuk testing\n",
    "        logging_dir=f\"./logs_fold_{fold + 1}\", # Direktori log unik per fold\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        save_strategy=IntervalStrategy.EPOCH,\n",
    "        report_to=\"none\" # Nonaktifkan reporting ke WANDB secara eksplisit\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    # Langkah 8 Memulai training model\n",
    "    trainer.train()\n",
    "\n",
    "    # Langkah 9 Evaluasi Model untuk fold saat ini\n",
    "    results = trainer.evaluate()\n",
    "    print(f\"Evaluation results for Fold {fold + 1}: {results}\")\n",
    "    all_fold_metrics.append(results)\n",
    "\n",
    "    # Simpan prediksi untuk kalkulasi metrik agregat nanti\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    predicted_labels = predictions.predictions.argmax(-1)\n",
    "    true_labels = test_dataset[\"labels\"]\n",
    "\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(true_labels)\n",
    "    all_texts.extend(test_dataset[\"text\"]) # *** PERBAIKAN: Kumpulkan teks dari test_dataset\n",
    "# ==============================================================================\n",
    "# AGREGASI HASIL CROSS-VALIDATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n===== Aggregating Cross-Validation Results =====\")\n",
    "\n",
    "# Hitung rata-rata metrik dari semua fold\n",
    "avg_eval_loss = np.mean([res['eval_loss'] for res in all_fold_metrics])\n",
    "print(f\"Average Eval Loss across {num_folds} folds: {avg_eval_loss:.4f}\")\n",
    "\n",
    "# Hitung metrik keseluruhan dari semua prediksi (dari semua fold)\n",
    "y_true_overall = np.array(all_true_labels)\n",
    "y_pred_overall = np.array(all_predictions)\n",
    "\n",
    "accuracy_overall = accuracy_score(y_true_overall, y_pred_overall)\n",
    "precision_macro_overall = precision_score(y_true_overall, y_pred_overall, average='macro')\n",
    "precision_weighted_overall = precision_score(y_true_overall, y_pred_overall, average='weighted')\n",
    "recall_macro_overall = recall_score(y_true_overall, y_pred_overall, average='macro')\n",
    "recall_weighted_overall = recall_score(y_true_overall, y_pred_overall, average='weighted')\n",
    "f1_macro_overall = f1_score(y_true_overall, y_pred_overall, average='macro')\n",
    "f1_weighted_overall = f1_score(y_true_overall, y_pred_overall, average='weighted')\n",
    "\n",
    "report_overall = classification_report(y_true_overall, y_pred_overall, target_names=list(label_map.keys()), output_dict=True)\n",
    "cm_overall = confusion_matrix(y_true_overall, y_pred_overall)\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {accuracy_overall:.4f}\")\n",
    "print(f\"Overall Precision (macro): {precision_macro_overall:.4f}\")\n",
    "print(f\"Overall Precision (weighted): {precision_weighted_overall:.4f}\")\n",
    "print(f\"Overall Recall (macro): {recall_macro_overall:.4f}\")\n",
    "print(f\"Overall Recall (weighted): {recall_weighted_overall:.4f}\")\n",
    "print(f\"Overall F1 Score (macro): {f1_macro_overall:.4f}\")\n",
    "print(f\"Overall F1 Score (weighted): {f1_weighted_overall:.4f}\")\n",
    "\n",
    "print(\"\\nOverall Confusion Matrix:\")\n",
    "print(cm_overall)\n",
    "\n",
    "# Simpan hasil agregat ke file Excel baru\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Buat DataFrame untuk menyimpan semua prediksi dari semua fold\n",
    "final_sentiment_results = pd.DataFrame({\n",
    "    \"text\": all_texts, # *** PERBAIKAN: Gunakan list all_texts yang sudah dikumpulkan\n",
    "    \"true_label\": all_true_labels,\n",
    "    \"predicted_label\": all_predictions\n",
    "})\n",
    "final_sentiment_results['true_label_text'] = final_sentiment_results['true_label'].map(reverse_label_map)\n",
    "final_sentiment_results['predicted_label_text'] = final_sentiment_results['predicted_label'].map(reverse_label_map)\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(\"sentiment_analysis_results_cross_validation.xlsx\") as writer:\n",
    "    final_sentiment_results.to_excel(writer, sheet_name=\"All Predictions\", index=False)\n",
    "\n",
    "    overall_metrics_df = pd.DataFrame({\n",
    "        'Metric': ['Overall Accuracy', 'Overall Precision (macro)', 'Overall Precision (weighted)',\n",
    "                   'Overall Recall (macro)', 'Overall Recall (weighted)',\n",
    "                   'Overall F1 Score (macro)', 'Overall F1 Score (weighted)', 'Average Eval Loss'],\n",
    "        'Value': [accuracy_overall, precision_macro_overall, precision_weighted_overall,\n",
    "                  recall_macro_overall, recall_weighted_overall,\n",
    "                  f1_macro_overall, f1_weighted_overall, avg_eval_loss]\n",
    "    })\n",
    "    overall_metrics_df.to_excel(writer, sheet_name=\"Overall CV Metrics\", index=False)\n",
    "\n",
    "    class_metrics_overall = pd.DataFrame()\n",
    "    for cls in report_overall:\n",
    "        if cls not in ['accuracy', 'macro avg', 'weighted avg']:\\\n",
    "            class_metrics_overall[cls] = pd.Series(report_overall[cls])\n",
    "    class_metrics_overall.to_excel(writer, sheet_name=\"Overall Class Metrics\", index=True)\n",
    "\n",
    "    # Tambahkan sheet untuk metrik per fold\n",
    "    fold_metrics_df = pd.DataFrame(all_fold_metrics)\n",
    "    fold_metrics_df.insert(0, 'Fold', range(1, num_folds + 1))\n",
    "    fold_metrics_df.to_excel(writer, sheet_name=\"Metrics Per Fold\", index=False)\n",
    "\n",
    "\n",
    "print(\"Cross-validation results saved to sentiment_analysis_results_cross_validation.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
